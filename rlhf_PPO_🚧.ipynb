{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKeVHd+x3KnPJXlgm2DViX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yyzhang2000/AI-Cookbook/blob/main/rlhf_PPO_%F0%9F%9A%A7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "L19jCBZnaF0b"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoModel, AutoModelForSequenceClassification, AutoTokenizer\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Union, Tuple\n",
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PromptDataset(Dataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            prompts,\n",
        "            tokenizer,\n",
        "            apply_chat_template = False\n",
        "    ):\n",
        "        self.prompts = prompts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.final_prompts = []\n",
        "\n",
        "        for prompt in prompts:\n",
        "            if apply_chat_template:\n",
        "                content = [{\n",
        "                    \"role\": 'user',\n",
        "                    'content': prompt\n",
        "                }]\n",
        "                prompt = self.tokenizer.apply_chat_template(\n",
        "                    content, tokenize = False, add_generation_prompt = True\n",
        "                )\n",
        "            else:\n",
        "                prompt = self.tokenizer.bos_token + prompt\n",
        "\n",
        "            print(prompt)\n",
        "            self.final_prompts.append(prompt)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.prompts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.final_prompts[idx]"
      ],
      "metadata": {
        "id": "D9EV48OjpJDo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super().__init__()\n",
        "\n",
        "        self.base_model = base_model\n",
        "        self.base_model.eval()\n",
        "\n",
        "        for p in self.base_model.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        self.value_head = nn.Linear(\n",
        "            base_model.config.hidden_size, 1\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, num_actions):\n",
        "        hidden_states = self.base_model(\n",
        "            input_ids,\n",
        "            attention_mask\n",
        "        ).last_hidden_state\n",
        "\n",
        "        value_model_output = self.value_head(hidden_states)\n",
        "        values = value_model_output.squeeze(-1)[:, -num_actions:]\n",
        "\n",
        "        return values\n"
      ],
      "metadata": {
        "id": "6euKMTsspmsw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_policy_loss(\n",
        "        log_prob,\n",
        "        old_log_prob,\n",
        "        advantages,\n",
        "        action_mask = None,\n",
        "        clip_eps = 0.2\n",
        "):\n",
        "    ratio = (log_prob - old_log_prob).exp()\n",
        "\n",
        "    surr1 = ratio * advantages\n",
        "    surr2 = ratio.clamp(\n",
        "        1.0 - clip_eps,\n",
        "        1.0 + clip_eps\n",
        "    ) * advantages\n",
        "\n",
        "    loss = -torch.min(\n",
        "        surr1, surr2\n",
        "    )\n",
        "\n",
        "    if action_mask is None:\n",
        "        return loss.mean(-1).mean()\n",
        "\n",
        "    return ((loss * action_mask).sum(-1) / action_mask.sum(-1)).mean()"
      ],
      "metadata": {
        "id": "_82-CEVEq0zw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_value_loss(\n",
        "    values,\n",
        "    old_values,\n",
        "    returns,\n",
        "    action_mask = None,\n",
        "    clip_eps: Optional[float] = None\n",
        "):\n",
        "    if clip_eps:\n",
        "        values_clipped = old_values + (\n",
        "            values - old_values\n",
        "        ).clamp(-clip_eps, clip_eps)\n",
        "\n",
        "        surr1 = (values_clipped - returns) ** 2\n",
        "        surr2 = (values - returns) ** 2\n",
        "\n",
        "        loss = torch.max(surr1, surr2)\n",
        "    else:\n",
        "        loss = (values - returns) ** 2\n",
        "\n",
        "\n",
        "    if not action_mask:\n",
        "        return loss.mean(-1).mean()\n",
        "\n",
        "    return ((loss * action_mask).sum(-1) / action_mask.sum(-1)).mean()\n"
      ],
      "metadata": {
        "id": "Uz2WCmgqrh2r"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperienceBuffer:\n",
        "    def __init__(self, limit):\n",
        "        self.limit = limit\n",
        "        self.buffer = []\n",
        "\n",
        "\n",
        "    def append(self, experiences):\n",
        "        batch = [{} for _ in range(len(experiences))]\n",
        "        keys = (\n",
        "            \"seqs\",\n",
        "            \"action_log_probs\",\n",
        "            \"values\",\n",
        "            \"returns\",\n",
        "            \"advantages\",\n",
        "            \"attention_mask\",\n",
        "            \"action_mask\",\n",
        "            \"num_actions\"\n",
        "        )\n",
        "\n",
        "        for key in keys:\n",
        "            for i, experience in enumerate(experiences):\n",
        "                value = getattr(experience, key)\n",
        "                batch[i][key] = value\n",
        "\n",
        "        self.buffer.extend(batch)\n",
        "\n",
        "        self.buffer.extend(batch)\n",
        "        if len(self.buffer) >= self.limit:\n",
        "            self.buffer = self.buffer[len(self.buffer)-self.limit:]\n",
        "\n",
        "    def get_batches(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "\n",
        "    def clear(self):\n",
        "        self.buffer = []\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.buffer[index]\n",
        ""
      ],
      "metadata": {
        "id": "092fZfMDsTjn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Samples:\n",
        "    seqs: torch.Tensor\n",
        "    attention_mask: Optional[torch.LongTensor]\n",
        "    action_mask: Optional[torch.BoolTensor]\n",
        "    num_actions: Union[int, torch.Tensor]\n",
        "    packed_seq_lens: Optional[torch.Tensor]\n",
        "    response_length: torch.Tensor\n",
        "    total_length: torch.Tensor\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Experience:\n",
        "    seqs: torch.Tensor\n",
        "    action_log_probs: torch.Tensor\n",
        "    values: torch.Tensor\n",
        "    returns: Optional[torch.Tensor]\n",
        "    advantages: Optional[torch.Tensor]\n",
        "    attention_mask: Optional[torch.LongTensor]\n",
        "    action_mask: Optional[torch.BoolTensor]\n",
        "    reward: torch.Tensor\n",
        "    response_length: torch.Tensor\n",
        "    total_length: torch.Tensor\n",
        "    num_actions: Union[int, torch.Tensor]\n",
        "    kl: Optional[torch.Tensor] = None"
      ],
      "metadata": {
        "id": "ZGwD1O80tM6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_approx_kl(\n",
        "        log_probs,\n",
        "        ref_log_probs,\n",
        "        action_mask\n",
        "):\n",
        "    log_ratio = log_probs.float() - ref_log_probs.float()\n",
        "    if action_mask:\n",
        "        log_ratio = log_ratio * action_mask\n",
        "\n",
        "    return log_ratio"
      ],
      "metadata": {
        "id": "dKiCeUXytU5C"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_advantages_and_returns(\n",
        "        values,\n",
        "        rewards,\n",
        "        action_mask,\n",
        "        gamma,\n",
        "        lambd\n",
        "):\n",
        "    last_gae_lam = 0\n",
        "    advantages_reversed = []\n",
        "    response_length = rewards.size(1)\n",
        "\n",
        "    if action_mask:\n",
        "        values = action_mask * values\n",
        "        rewards = action_mask * rewards\n",
        "\n",
        "    for t in reversed(range(response_length)):\n",
        "        next_values = values[:, t + 1] if t < response_length - 1 else 0.0\n",
        "\n",
        "        delta = rewards[:, t] + gamma * next_values - values[:t]\n",
        "        last_gae_lam = delta + gamma * lambd * last_gae_lam\n",
        "        advantages_reversed.append(last_gae_lam)\n",
        "\n",
        "    advantages = torch.stack(advantages_reversed[::-1], dim=1)\n",
        "    returns = advantages + values\n",
        "\n",
        "    return advantages.detach(), returns\n"
      ],
      "metadata": {
        "id": "eHx9flsttpUY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_samples(\n",
        "        prompts,\n",
        "        model,\n",
        "        max_length,\n",
        "        max_new_tokens,\n",
        "        n_samples_per_prompt,\n",
        "        micro_rollout_batch_size\n",
        "):\n",
        "    samples_list = []\n",
        "    model.eval()\n",
        "\n",
        "    all_prompts = sum([\n",
        "        [prompt] * n_samples_per_prompt for prompt in prompts\n",
        "    ], [])\n",
        "\n",
        "    for i in range(0, len(all_prompts), micro_rollout_batch_size):\n",
        "        prompts = all_prompts[i:i+micro_rollout_batch_size]\n",
        "        inputs = actor_tokenizer(prompts, padding='max_length', max_length=max_length, truncation=True, return_tensors='pt')\n",
        "        input_ids = inputs['input_ids']\n",
        "        seqs = model.generate(**inputs.to(device),\n",
        "                            max_new_tokens = max_new_tokens,\n",
        "                            eos_token_id = eos_token_id,\n",
        "                            pad_token_id = pad_token_id)\n",
        "        if seqs.size(1) >= max_new_tokens + max_length:\n",
        "            seqs = seqs[:, :max_new_tokens + max_length]\n",
        "        else:\n",
        "            seqs = torch.cat([seqs, torch.full((seqs.size(0), max_new_tokens + max_length - seqs.size(1)), fill_value=pad_token_id, device=seqs.device)], dim=1)\n",
        "\n",
        "        attention_mask = (seqs.ne(pad_token_id)).to(dtype=torch.long)\n",
        "        ans = seqs[:, input_ids.size(1):]\n",
        "        action_mask = (ans.ne(eos_token_id) & ans.ne(pad_token_id)).to(dtype=torch.long)\n",
        "\n",
        "\n",
        "        samples = Samples(\n",
        "            seqs=seqs,\n",
        "            attention_mask=attention_mask,\n",
        "            action_mask=action_mask,\n",
        "            num_actions=action_mask.size(1),\n",
        "            packed_seq_lens=None,\n",
        "            response_length=action_mask.float().sum(dim=-1),\n",
        "            total_length=attention_mask.float().sum(dim=-1),\n",
        "        )\n",
        "        samples_list.append(samples)\n",
        "\n",
        "    return samples_list"
      ],
      "metadata": {
        "id": "9aSnatJozddK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x2pl6xOl0DHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class BufferItem:\n",
        "    seqs: torch.Tensor\n",
        "    action_log_probs: torch.Tensor\n",
        "    values: torch.Tensor\n",
        "    returns: torch.Tensor\n",
        "    advantages: torch.Tensor\n",
        "    attention_mask: torch.Tensor\n",
        "    action_mask: torch.Tensor\n",
        "    num_actions: Union[int, torch.Tensor]"
      ],
      "metadata": {
        "id": "eZcwrzUo0C75"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(\n",
        "        experience,\n",
        "        steps\n",
        "):\n",
        "    actor_model.train()\n",
        "    optimizer_actor.zero_grad()\n",
        "\n",
        "    sequences = experience.seqs\n",
        "    old_action_log_probs = experience.action_log_probs\n",
        "    advantages = experience.advantages\n",
        "    num_actions = experience.num_actions\n",
        "    attention_mask = experience.attention_mask\n",
        "    action_mask = experience.action_mask\n",
        "    old_values = experience.values\n",
        "    returns = experience.returns\n",
        "\n",
        "    logits = actor_model(\n",
        "        sequences,\n",
        "        attention_mask=attention_mask\n",
        "    ).logits\n",
        "\n",
        "    log_probs = F.log_softmax(\n",
        "        logits[:, :-1, :], dim = -1\n",
        "    )"
      ],
      "metadata": {
        "id": "LT_Z347a0I-T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}