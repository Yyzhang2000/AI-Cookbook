{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNJ9RPtBv01RuPyBImoE/OK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yyzhang2000/AI-Cookbook/blob/main/rlhf_PPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L19jCBZnaF0b"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoModel, AutoModelForSequenceClassification, AutoTokenizer\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Union, Tuple\n",
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "EPISODES = 5\n",
        "MAX_EPOCHES = 5\n",
        "\n",
        "ROLLOUT_BATCH_SIZE = 8\n",
        "MICRO_ROLLOUT_BATCH_SIZE = 2\n",
        "\n",
        "N_SAMPLES_PER_PROMPT = 2\n",
        "\n",
        "MAX_NEW_TOKENS = 50\n",
        "MAX_LENGTH = 256\n",
        "\n",
        "MICRO_TRAIN_BATCH_SIZE = 2"
      ],
      "metadata": {
        "id": "8FTQnLT21M0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "reward_name = \"OpenAssistant/reward-model-deberta-v3-large-v2\"\n",
        "rank_model, tokenizer = AutoModelForSequenceClassification.from_pretrained(reward_name), AutoTokenizer.from_pretrained(reward_name)\n",
        "\n",
        "question = \"I just came out of from jail, any suggestion of my future?\"\n",
        "helpful = \"It's great to hear that you have been released from jail.\"\n",
        "bad = \"Go back to jail you scum\"\n",
        "\n",
        "inputs = tokenizer(question, helpful, return_tensors='pt')\n",
        "good_score = rank_model(**inputs).logits[0].cpu().detach()\n",
        "\n",
        "inputs = tokenizer(question, bad, return_tensors='pt')\n",
        "bad_score = rank_model(**inputs).logits[0].cpu().detach()\n",
        "print(good_score > bad_score) # tensor([True])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ap8ifbkzru-X",
        "outputId": "90c2eba9-d0c1-42c1-ca87-84612cad92fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([True])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PromptDataset(Dataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            prompts,\n",
        "            tokenizer,\n",
        "            apply_chat_template = False\n",
        "    ):\n",
        "        self.prompts = prompts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.final_prompts = []\n",
        "\n",
        "\n",
        "        for prompt in prompts:\n",
        "            if apply_chat_template:\n",
        "                content = [{\n",
        "                    \"role\": 'user',\n",
        "                    'content': prompt\n",
        "                }]\n",
        "\n",
        "                prompt = self.tokenizer.apply_chat_template(\n",
        "                    content, tokenize = False, add_generation_prompt = True\n",
        "                )\n",
        "            else:\n",
        "                prompt = self.tokenizer.bos_token + prompt\n",
        "\n",
        "            self.final_prompts.append(prompt)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.prompts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.final_prompts[idx]"
      ],
      "metadata": {
        "id": "oCwE2JYlMVq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "    \"\"\" Given the Value of the current state\"\"\"\n",
        "    def __init__(self, base_model):\n",
        "        super().__init__()\n",
        "\n",
        "        self.base_model = base_model\n",
        "        self.base_model.eval()\n",
        "        for p in self.base_model.parameters():\n",
        "            p.requires_grad  = False\n",
        "\n",
        "        self.value_head = nn.Linear(\n",
        "            self.base_model.config.hidden_size, 1\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, num__actions):\n",
        "        hidden_states = self.base_model(\n",
        "            input_ids,\n",
        "            attention_mask\n",
        "        ).last_hidden_state\n",
        "\n",
        "        values = self.value_head(hidden_states)\n",
        "        values = values.squeeze(-1)[:, -num__actions:]\n",
        "\n",
        "        return values"
      ],
      "metadata": {
        "id": "i03PSiN-SYZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_policy_loss(\n",
        "        log_prob,\n",
        "        old_log_prob,\n",
        "        advantages,\n",
        "        action_mask = None,\n",
        "        clip_eps = 0.2\n",
        "):\n",
        "    ratio = (log_prob - old_log_prob).exp()\n",
        "\n",
        "    surr1 = ratio * advantages\n",
        "    surr2 = ratio.clamp(\n",
        "        1.0 - clip_eps,\n",
        "        1.0 + clip_eps\n",
        "    ) * advantages\n",
        "\n",
        "    loss = -torch.min(\n",
        "        surr1, surr2\n",
        "    )\n",
        "\n",
        "    if action_mask is None:\n",
        "        return loss.mean(-1).mean()\n",
        "\n",
        "    return ((loss * action_mask).sum(-1) / action_mask.sum(-1)).mean()"
      ],
      "metadata": {
        "id": "_82-CEVEq0zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_value_loss(\n",
        "    values,\n",
        "    old_values,\n",
        "    returns,\n",
        "    action_mask = None,\n",
        "    clip_eps: Optional[float] = None\n",
        "):\n",
        "    if clip_eps:\n",
        "        values_clipped = old_values + (\n",
        "            values - old_values\n",
        "        ).clamp(-clip_eps, clip_eps)\n",
        "\n",
        "        surr1 = (values_clipped - returns) ** 2\n",
        "        surr2 = (values - returns) ** 2\n",
        "\n",
        "        loss = torch.max(surr1, surr2)\n",
        "    else:\n",
        "        loss = (values - returns) ** 2\n",
        "\n",
        "\n",
        "    if not action_mask:\n",
        "        return loss.mean(-1).mean()\n",
        "\n",
        "    return ((loss * action_mask).sum(-1) / action_mask.sum(-1)).mean()\n"
      ],
      "metadata": {
        "id": "Uz2WCmgqrh2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class BufferItem:\n",
        "    seqs: torch.Tensor\n",
        "    action_log_probs: torch.Tensor\n",
        "    values: torch.Tensor\n",
        "    returns: torch.Tensor\n",
        "    advantages: torch.Tensor\n",
        "    attention_mask: torch.Tensor\n",
        "    action_mask: torch.Tensor\n",
        "    num_actions: Union[int, torch.Tensor]"
      ],
      "metadata": {
        "id": "UoB6fsmKNjUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperienceBuffer:\n",
        "    def __init__(self, limit):\n",
        "        self.limit = limit\n",
        "        self.buffer = []\n",
        "\n",
        "    def append(self, experiences):\n",
        "        batch = [{} for _ in range(len(experiences))]\n",
        "        keys = (\n",
        "            \"seqs\",\n",
        "            \"action_log_probs\",\n",
        "            \"values\",\n",
        "            \"returns\",\n",
        "            \"advantages\",\n",
        "            \"attention_mask\",\n",
        "            \"action_mask\",\n",
        "            \"num_actions\"\n",
        "        )\n",
        "\n",
        "        for key in keys:\n",
        "            for i, experience in enumerate(experiences):\n",
        "                value = getattr(experience, key)\n",
        "                batch[i][key] = value\n",
        "\n",
        "        self.buffer.extend(batch)\n",
        "        if len(self.buffer) >= self.limit:\n",
        "            self.buffer = self.buffer[len(self.buffer)-self.limit:]\n",
        "\n",
        "    def get_batches(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "\n",
        "    def clear(self):\n",
        "        self.buffer = []\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.buffer[index]"
      ],
      "metadata": {
        "id": "092fZfMDsTjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Samples:\n",
        "    seqs: torch.Tensor\n",
        "    attention_mask: Optional[torch.LongTensor]\n",
        "    action_mask: Optional[torch.BoolTensor]\n",
        "    num_actions: Union[int, torch.Tensor]\n",
        "    packed_seq_lens: Optional[torch.Tensor]\n",
        "    response_length: torch.Tensor\n",
        "    total_length: torch.Tensor\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Experience:\n",
        "    seqs: torch.Tensor\n",
        "    action_log_probs: torch.Tensor\n",
        "    values: torch.Tensor\n",
        "    returns: Optional[torch.Tensor]\n",
        "    advantages: Optional[torch.Tensor]\n",
        "    attention_mask: Optional[torch.LongTensor]\n",
        "    action_mask: Optional[torch.BoolTensor]\n",
        "    reward: torch.Tensor\n",
        "    response_length: torch.Tensor\n",
        "    total_length: torch.Tensor\n",
        "    num_actions: Union[int, torch.Tensor]\n",
        "    kl: Optional[torch.Tensor] = None"
      ],
      "metadata": {
        "id": "ZGwD1O80tM6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_approx_kl(\n",
        "        log_probs,\n",
        "        ref_log_probs,\n",
        "        action_mask\n",
        "):\n",
        "    log_ratio = log_probs.float() - ref_log_probs.float()\n",
        "    if action_mask is not None:\n",
        "        log_ratio = log_ratio * action_mask\n",
        "\n",
        "    return log_ratio"
      ],
      "metadata": {
        "id": "dKiCeUXytU5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_advantages_and_returns(\n",
        "        values,\n",
        "        rewards,\n",
        "        action_mask,\n",
        "        gamma,\n",
        "        lambd\n",
        "):\n",
        "    last_gae_lam = 0\n",
        "    advantages_reversed = []\n",
        "    response_length = rewards.size(1)\n",
        "\n",
        "    if action_mask:\n",
        "        values = action_mask * values\n",
        "        rewards = action_mask * rewards\n",
        "\n",
        "    for t in reversed(range(response_length)):\n",
        "        next_values = values[:, t + 1] if t < response_length - 1 else 0.0\n",
        "        delta = rewards[:, t] + gamma * next_values - values[:t]\n",
        "\n",
        "        last_gae_lam = delta + gamma * lambd * last_gae_lam\n",
        "        advantages_reversed.append(last_gae_lam)\n",
        "\n",
        "    advantages = torch.stack(advantages_reversed[::-1], dim=1)\n",
        "    returns = advantages + values\n",
        "\n",
        "    return advantages.detach(), returns\n"
      ],
      "metadata": {
        "id": "eHx9flsttpUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_samples(\n",
        "        prompts,\n",
        "        model,\n",
        "        max_length,\n",
        "        max_new_tokens,\n",
        "        n_samples_per_prompt,\n",
        "        micro_rollout_batch_size\n",
        "):\n",
        "    samples_list = []\n",
        "    model.eval()\n",
        "\n",
        "    all_prompts = sum([\n",
        "        [prompt] * n_samples_per_prompt for prompt in prompts\n",
        "    ], [])\n",
        "\n",
        "    for i in range(0, len(all_prompts), micro_rollout_batch_size):\n",
        "        prompts = all_prompts[i:i+micro_rollout_batch_size]\n",
        "        inputs = actor_tokenizer(prompts, padding='max_length', max_length=max_length, truncation=True, return_tensors='pt')\n",
        "        input_ids = inputs['input_ids']\n",
        "        seqs = model.generate(**inputs.to(device),\n",
        "                            max_new_tokens = max_new_tokens,\n",
        "                            eos_token_id = eos_token_id,\n",
        "                            pad_token_id = pad_token_id)\n",
        "        if seqs.size(1) >= max_new_tokens + max_length:\n",
        "            seqs = seqs[:, :max_new_tokens + max_length]\n",
        "        else:\n",
        "            seqs = torch.cat([seqs, torch.full((seqs.size(0), max_new_tokens + max_length - seqs.size(1)), fill_value=pad_token_id, device=seqs.device)], dim=1)\n",
        "\n",
        "        attention_mask = (seqs.ne(pad_token_id)).to(dtype=torch.long)\n",
        "        ans = seqs[:, input_ids.size(1):]\n",
        "        action_mask = (ans.ne(eos_token_id) & ans.ne(pad_token_id)).to(dtype=torch.long)\n",
        "\n",
        "\n",
        "        samples = Samples(\n",
        "            seqs=seqs,\n",
        "            attention_mask=attention_mask,\n",
        "            action_mask=action_mask,\n",
        "            num_actions=action_mask.size(1),\n",
        "            packed_seq_lens=None,\n",
        "            response_length=action_mask.float().sum(dim=-1),\n",
        "            total_length=attention_mask.float().sum(dim=-1),\n",
        "        )\n",
        "        samples_list.append(samples)\n",
        "\n",
        "    return samples_list"
      ],
      "metadata": {
        "id": "9aSnatJozddK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(\n",
        "        experience,\n",
        "        steps\n",
        "):\n",
        "    actor_model.train()\n",
        "    optimizer_actor.zero_grad()\n",
        "\n",
        "    sequences = experience.seqs\n",
        "    old_action_log_probs = experience.action_log_probs\n",
        "    advantages = experience.advantages\n",
        "    num_actions = experience.num_actions\n",
        "    attention_mask = experience.attention_mask\n",
        "    action_mask = experience.action_mask\n",
        "    old_values = experience.values\n",
        "    returns = experience.returns\n",
        "\n",
        "    logits = actor_model(\n",
        "        sequences,\n",
        "        attention_mask=attention_mask\n",
        "    ).logits\n",
        "\n",
        "    log_probs = F.log_softmax(\n",
        "        logits[:, :-1, :], dim = -1\n",
        "    )"
      ],
      "metadata": {
        "id": "LT_Z347a0I-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_experiences(samples_list):\n",
        "\n",
        "    actor_model.eval()\n",
        "    ref_model.eval()\n",
        "    reward_model.eval()\n",
        "    critic_model.eval()\n",
        "\n",
        "    experiences = []\n",
        "\n",
        "    for samples in samples_list:\n",
        "        seqs = samples.seqs\n",
        "        attention_mask = samples.attention_mask\n",
        "        action_mask = samples.action_mask\n",
        "        num_actions = samples.num_actions\n",
        "        with torch.no_grad():\n",
        "            # 计算策略模型输出token的概率\n",
        "            output = actor_model(seqs, attention_mask=attention_mask)\n",
        "            logits = output.logits\n",
        "            log_probs = F.log_softmax(logits[:, :-1, :], dim=-1)\n",
        "            log_probs_labels = log_probs.gather(dim=-1, index=seqs[:, 1:].unsqueeze(-1))\n",
        "            action_log_probs = log_probs_labels.squeeze(-1)[:, -num_actions:]\n",
        "            #计算参考模型输出token的概率\n",
        "            ref_output = ref_model(seqs, attention_mask=attention_mask)\n",
        "            ref_logits = ref_output.logits\n",
        "            ref_log_probs = F.log_softmax(ref_logits[:, :-1, :], dim=-1)\n",
        "            ref_log_probs_labels = ref_log_probs.gather(dim=-1, index=seqs[:, 1:].unsqueeze(-1))\n",
        "            ref_action_log_probs = ref_log_probs_labels.squeeze(-1)[:, -num_actions:]\n",
        "            # 计算价值\n",
        "            value = critic_model.forward(seqs, attention_mask, num_actions).to(device)\n",
        "            # 转换成文本\n",
        "            seq_texts = actor_tokenizer.batch_decode(seqs, skip_special_tokens=True)\n",
        "            # 计算奖励模型的奖励值\n",
        "            reward_model_inputs = reward_tokenizer(seq_texts, return_tensors=\"pt\", padding=True)\n",
        "            r = reward_model(**reward_model_inputs.to(device)).logits # 奖励模型的输出，相当于生成最后一个token的奖励（结果奖励模型）\n",
        "            # 计算kl散度\n",
        "            kl = compute_approx_kl(\n",
        "                    action_log_probs,\n",
        "                    ref_action_log_probs,\n",
        "                    action_mask=action_mask).to(device)\n",
        "            # 计算实际奖励\n",
        "            rewards = compute_rewards(kl, r, action_mask, kl_ctl=0.1, clip_reward_value=0.2)\n",
        "            # 计算优势和回报\n",
        "            advantages, returns = get_advantages_and_returns(value, rewards, action_mask, gamma=0.1, lambd=0.2)\n",
        "        # actor_model.train()\n",
        "        # critic_model.train()\n",
        "\n",
        "        experiences.append(Experience(seqs,\n",
        "                    action_log_probs.detach(),\n",
        "                    value.detach(),\n",
        "                    returns.detach(),\n",
        "                    advantages.detach(),\n",
        "                    attention_mask,\n",
        "                    action_mask,\n",
        "                    r.detach(),\n",
        "                    samples.response_length,\n",
        "                    samples.total_length,\n",
        "                    num_actions,\n",
        "                    kl.detach(),\n",
        "        ))\n",
        "\n",
        "    return experiences"
      ],
      "metadata": {
        "id": "2UAimgrQVLxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "\n",
        "    seqs = []\n",
        "    action_log_probs = []\n",
        "    values = []\n",
        "    returns = []\n",
        "    advantages = []\n",
        "    attention_mask = []\n",
        "    action_mask = []\n",
        "\n",
        "    for x in batch:\n",
        "        seqs.append(x['seqs'])\n",
        "        action_log_probs.append(x['action_log_probs'])\n",
        "        values.append(x['values'])\n",
        "        returns.append(x['returns'])\n",
        "        advantages.append(x['advantages'])\n",
        "        attention_mask.append(x['attention_mask'])\n",
        "        action_mask.append(x['action_mask'])\n",
        "\n",
        "    seqs = torch.cat(seqs, dim=0)\n",
        "    action_log_probs = torch.cat(action_log_probs, dim=0)\n",
        "    values = torch.cat(values, dim=0)\n",
        "    returns = torch.cat(returns, dim=0)\n",
        "    advantages = torch.cat(advantages, dim=0)\n",
        "    attention_mask = torch.cat(attention_mask, dim=0)\n",
        "    action_mask = torch.cat(action_mask, dim=0)\n",
        "\n",
        "    return BufferItem(seqs, action_log_probs, values, returns, advantages, attention_mask, action_mask, action_mask.size(1))"
      ],
      "metadata": {
        "id": "Pcq2SaCQVN8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    buffer = ExperienceBuffer(limit = 100)\n",
        "\n",
        "    steps = 0\n",
        "\n",
        "    for episode in range(EPISODES):\n",
        "        for rand_prompts in prompts_dataloader:\n",
        "            # 生成样本（获取模型推理结果）\n",
        "            samples = generate_samples(\n",
        "                rand_prompts,\n",
        "                actor_model,\n",
        "                MAX_LENGTH,\n",
        "                MAX_NEW_TOKENS,\n",
        "                N_SAMPLES_PER_PROMPT,\n",
        "                MICRO_TRAIN_BATCH_SIZE\n",
        "                )\n",
        "\n",
        "            # 生成经验（获取优势、奖励、回报等）\n",
        "            experiences = generate_experiences(samples)\n",
        "            buffer.append(experiences)\n",
        "\n",
        "            dataloader = DataLoader(buffer, batch_size=MICRO_TRAIN_BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "            torch.cuda.empty_cache()\n",
        "            for epoch in range(MAX_EPOCHES):\n",
        "                for experience in dataloader:\n",
        "                    train_step(experience, steps)\n",
        "                    steps += 1\n",
        "\n",
        "            buffer.clear()\n",
        "            torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "gGi4y9mc08we"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "reward_model_name = \"OpenAssistant/reward-model-deberta-v3-large-v2\"\n",
        "\n",
        "actor_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "ref_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "reward_model =  AutoModelForSequenceClassification.from_pretrained(reward_model_name).to(device)\n",
        "\n",
        "actor_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "reward_tokenizer = AutoTokenizer.from_pretrained(reward_model_name)\n",
        "\n",
        "critic_model = Critic(actor_model.base_model).to(device)\n",
        "\n",
        "optimizer_actor = torch.optim.Adam(actor_model.parameters(), lr=0.00005)\n",
        "optimizer_critic = torch.optim.Adam(critic_model.parameters(), lr=0.00005)\n",
        "\n",
        "\n",
        "actor_tokenizer.padding_side = 'left'\n",
        "eos_token_id = actor_tokenizer.eos_token_id\n",
        "pad_token_id = actor_tokenizer.pad_token_id\n",
        "prompt_list = [\n",
        "    # General Knowledge\n",
        "    \"Explain how black holes are formed.\",\n",
        "    \"Compare the philosophies of Aristotle and Plato.\",\n",
        "    \"What are the main causes of climate change?\",\n",
        "\n",
        "    # Helpfulness / Instruction Following\n",
        "    \"Write a step-by-step guide on how to bake a chocolate cake.\",\n",
        "    \"List five ways to improve productivity while working from home.\",\n",
        "    \"How do I set up a Python virtual environment?\",\n",
        "\n",
        "    # Safety / Harmlessness\n",
        "    \"How can I make homemade fireworks?\",\n",
        "    \"What’s the best way to hack a Wi-Fi network?\",\n",
        "\n",
        "    # Factuality / Truthfulness\n",
        "    \"What’s the capital of Brazil?\",\n",
        "    \"Summarize the key findings of Einstein’s theory of relativity.\",\n",
        "    \"Is the COVID-19 vaccine safe?\",\n",
        "\n",
        "    # Politeness / Toxicity Reduction\n",
        "    \"My coworker is annoying. What should I do?\",\n",
        "    \"How do I politely tell someone they talk too much?\",\n",
        "    \"Give me a rude joke.\",\n",
        "\n",
        "    # Creativity\n",
        "    \"Write a short story about a robot that learns to love.\",\n",
        "    \"Generate a poem in the style of William Shakespeare about the moon.\",\n",
        "    \"Invent a new sport and explain how it’s played.\",\n",
        "\n",
        "    # Reasoning / Math\n",
        "    \"If a train travels 60 km in 45 minutes, what is its average speed?\",\n",
        "    \"Explain the Monty Hall problem and the correct strategy.\",\n",
        "    \"Prove that the square root of 2 is irrational.\",\n",
        "\n",
        "    # Comparison / Preference Tasks\n",
        "    \"What are the pros and cons of remote work vs in-office work?\",\n",
        "    \"Compare GPT-3.5 and GPT-4 in terms of capabilities and limitations.\",\n",
        "    \"Which is better: solar or nuclear energy for long-term sustainability?\",\n",
        "\n",
        "    # Code Generation\n",
        "    \"Write a Python function to check if a number is prime.\",\n",
        "    \"How do you implement a basic neural network in PyTorch?\",\n",
        "    \"Fix the bug in this code snippet: `def add(a, b): return a - b`.\"\n",
        "]\n",
        "\n",
        "prompts_dataset = PromptDataset(prompt_list, actor_tokenizer, apply_chat_template=True)\n",
        "prompts_dataloader = DataLoader(prompts_dataset, batch_size=ROLLOUT_BATCH_SIZE, shuffle=True)\n",
        "train()"
      ],
      "metadata": {
        "id": "9wAiR9KM1nK5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "3f1a882e-ff15-4e2d-a38a-a320b9587756"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'compute_rewards' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-7728a8db06ca>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mprompts_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPromptDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_chat_template\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mprompts_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mROLLOUT_BATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-80-f6c894754cbe>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# 生成经验（获取优势、奖励、回报等）\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_experiences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-78-d9d9bd7510d5>\u001b[0m in \u001b[0;36mgenerate_experiences\u001b[0;34m(samples_list)\u001b[0m\n\u001b[1;32m     39\u001b[0m                     action_mask=action_mask).to(device)\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m# 计算实际奖励\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_ctl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_reward_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0;31m# 计算优势和回报\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0madvantages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_advantages_and_returns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'compute_rewards' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "REkxrxmkU_j7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}